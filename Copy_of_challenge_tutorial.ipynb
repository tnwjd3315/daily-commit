{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tnwjd3315/daily-commit/blob/main/Copy_of_challenge_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FMW_2xvO8A8g",
      "metadata": {
        "id": "FMW_2xvO8A8g"
      },
      "source": [
        "### Colab Instructions\n",
        "\n",
        "- **Files and Libraries Are Handled Automatically**  \n",
        "  The dataset, model checkpoint, `compute_cost.py`, and all required libraries will be automatically downloaded and installed when you run the notebook.\n",
        "\n",
        "- **Enable GPU Acceleration**  \n",
        "  Go to **Runtime** → **Change runtime type**, and select **GPU** (e.g., **T4 GPU**) as the hardware accelerator.\n",
        "\n",
        "- **Run the Notebook**  \n",
        "  Click **Runtime** → **Run all** to execute all cells sequentially.\n",
        "\n",
        "- **Logging with Weights and Biases**  \n",
        "  The notebook will prompt you to paste your API token. You can obtain the token by creating a free account at [Weights and Biases](https://wandb.ai/site/).\n",
        "\n",
        "- **Working Directory**  \n",
        "  The working directory is set to `/content`.\n",
        "\n",
        "- **Runtime Duration**  \n",
        "  Running the full notebook will take approximately 45 minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14c24699-ec84-4c8c-9cd6-0f571cccccdc",
      "metadata": {
        "id": "14c24699-ec84-4c8c-9cd6-0f571cccccdc"
      },
      "source": [
        "# Advanced Sound Event Detection Tutorial\n",
        "\n",
        "In this tutorial, you will learn how to:\n",
        "- Create a train/validation/test split  \n",
        "- Evaluate classifiers using standard metrics (e.g., precision, recall, f1-score)  \n",
        "- Compute segment-level costs based on classifier output  \n",
        "- Establish a simple baseline\n",
        "- Train and assess a logistic regression model on audio embeddings  \n",
        "- Build and evaluate a bidirectional RNN for sequence modeling  \n",
        "- Compare cost performance across baseline, logistic regression, and RNN models on the test set\n",
        "- Run inference on the customer's secret test set and store the predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "G2xwuRlfVcSE",
      "metadata": {
        "id": "G2xwuRlfVcSE"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install --quiet numpy pandas matplotlib scikit-learn torch torchvision torchaudio pytorch-lightning wandb rich ipywidgets tabulate tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27022428-d8fb-423e-8573-b5e56ba48f6c",
      "metadata": {
        "id": "27022428-d8fb-423e-8573-b5e56ba48f6c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tabulate import tabulate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    balanced_accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score\n",
        ")\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import (\n",
        "    ModelCheckpoint,\n",
        "    EarlyStopping,\n",
        "    LearningRateMonitor,\n",
        "    RichProgressBar\n",
        ")\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from tqdm import tqdm\n",
        "from huggingface_hub import snapshot_download, hf_hub_download\n",
        "import zipfile\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-e6IfhMP9RAl",
      "metadata": {
        "id": "-e6IfhMP9RAl"
      },
      "outputs": [],
      "source": [
        "# download the compute_cost.py file\n",
        "pyfile_path = hf_hub_download(\n",
        "    repo_id=\"fschmid56/mlpc2025_dataset\",\n",
        "    filename=\"compute_cost.py\",\n",
        "    repo_type=\"dataset\"\n",
        ")\n",
        "\n",
        "# move to current working directory (/content)\n",
        "shutil.copy(pyfile_path, os.getcwd() + \"/compute_cost.py\")\n",
        "\n",
        "# import required functions\n",
        "from compute_cost import CLASSES as TARGET_CLASSES\n",
        "from compute_cost import (\n",
        "    aggregate_targets,\n",
        "    get_ground_truth_df,\n",
        "    get_segment_prediction_df,\n",
        "    check_dataframe,\n",
        "    total_cost\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1aouGyDxj8sm",
      "metadata": {
        "id": "1aouGyDxj8sm"
      },
      "source": [
        "## Download and prepare MLPC2025 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_p8lV1rdZSt9",
      "metadata": {
        "id": "_p8lV1rdZSt9"
      },
      "outputs": [],
      "source": [
        "# Step 1: Download the ZIP file from HF Hub\n",
        "zip_path = hf_hub_download(\n",
        "    repo_id=\"fschmid56/mlpc2025_dataset\",   # your dataset repo\n",
        "    filename=\"mlpc2025_dataset.zip\",        # your uploaded ZIP file\n",
        "    repo_type=\"dataset\"                     # specify that it's a dataset repo\n",
        ")\n",
        "\n",
        "print(f\"✅ ZIP downloaded: {zip_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RqDJwV0RiYMu",
      "metadata": {
        "id": "RqDJwV0RiYMu"
      },
      "outputs": [],
      "source": [
        "# Step 2: Extract the ZIP\n",
        "extract_path = \"/content/mlpc2025_dataset\"\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Check if already extracted\n",
        "if not os.path.exists(os.path.join(extract_path, \"data\")):  # assuming 'data/' is inside the zip\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(f\"✅ Dataset extracted to {extract_path}\")\n",
        "else:\n",
        "    print(f\"✅ Dataset already extracted at {extract_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "urfcka--ipMc",
      "metadata": {
        "id": "urfcka--ipMc"
      },
      "outputs": [],
      "source": [
        "# Step 3: Set your DATASET_PATH\n",
        "DATASET_PATH = os.path.join(extract_path, \"data\")  # because you zipped the 'data' folder\n",
        "print(f\"✅ DATASET_PATH set to {DATASET_PATH}\")\n",
        "\n",
        "# Quick check\n",
        "print(\"Files in DATASET_PATH:\", os.listdir(DATASET_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4dea22b-5117-4588-8ce8-d38f4b6ba6d8",
      "metadata": {
        "id": "c4dea22b-5117-4588-8ce8-d38f4b6ba6d8"
      },
      "outputs": [],
      "source": [
        "METADATA_CSV = os.path.join(DATASET_PATH, 'metadata.csv')\n",
        "ANNOTATIONS_CSV = os.path.join(DATASET_PATH, 'annotations.csv')\n",
        "AUDIO_DIR = os.path.join(DATASET_PATH, 'audio')\n",
        "AUDIO_FEATURES_DIR = os.path.join(DATASET_PATH, 'audio_features')\n",
        "LABELS_DIR = os.path.join(DATASET_PATH, 'labels')\n",
        "\n",
        "METADATA = pd.read_csv(METADATA_CSV)\n",
        "DEV_SET_FILES = METADATA['filename']\n",
        "\n",
        "CUSTOMER_DATASET_PATH = os.path.join(DATASET_PATH, 'customer_test_data')\n",
        "CUSTOMER_AUDIO_DIR = os.path.join(CUSTOMER_DATASET_PATH, 'audio')\n",
        "CUSTOMER_AUDIO_FEATURES_DIR = os.path.join(CUSTOMER_DATASET_PATH, 'audio_features')\n",
        "CUSTOMER_METADATA_CSV = os.path.join(CUSTOMER_DATASET_PATH, 'metadata.csv')\n",
        "CUSTOMER_METADATA = pd.read_csv(CUSTOMER_METADATA_CSV)\n",
        "\n",
        "DATA_SUBSAMPLE = 3000  # works with available RAM in Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b68eb71-5949-4b39-9620-d6ea56d62e22",
      "metadata": {
        "id": "5b68eb71-5949-4b39-9620-d6ea56d62e22"
      },
      "source": [
        "## Create the Data Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91a26758-e613-49d0-8966-8d2f54aefc93",
      "metadata": {
        "id": "91a26758-e613-49d0-8966-8d2f54aefc93"
      },
      "outputs": [],
      "source": [
        "def read_files(file_names, classes, features_dir=AUDIO_FEATURES_DIR, labels_dir=LABELS_DIR):\n",
        "    \"\"\"\n",
        "    Loads features and binary labels for a list of files.\n",
        "\n",
        "    Returns:\n",
        "        X: list of np.ndarrays, each of shape (num_frames, num_features)\n",
        "        Y: dict of lists of np.ndarrays, each of shape (num_frames,)\n",
        "    \"\"\"\n",
        "    X = []\n",
        "    Y = {c: [] for c in classes} if labels_dir is not None else None\n",
        "\n",
        "    for fname in file_names:\n",
        "        base = os.path.splitext(fname)[0]\n",
        "\n",
        "        # Load features\n",
        "        feat_path = os.path.join(features_dir, base + '.npz')\n",
        "        features = np.load(feat_path)['embeddings']  # shape: (T, D)\n",
        "        X.append(features)\n",
        "\n",
        "        if labels_dir is not None:\n",
        "            # Load labels\n",
        "            label_path = os.path.join(labels_dir, base + '_labels.npz')\n",
        "            labels = np.load(label_path)\n",
        "\n",
        "            for c in classes:\n",
        "                label_array = labels[c]  # shape: (T, num_annotators)\n",
        "                binary_labels = (np.max(label_array, axis=1) > 0).astype(int)\n",
        "                Y[c].append(binary_labels)  # shape: (T,)\n",
        "\n",
        "    return X, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5508d1b1-2fea-434b-b0ee-11d343b2f11a",
      "metadata": {
        "id": "5508d1b1-2fea-434b-b0ee-11d343b2f11a"
      },
      "outputs": [],
      "source": [
        "# Get filenames for split based on filenames\n",
        "all_files = DEV_SET_FILES.unique()\n",
        "\n",
        "# First split: 60% train, 40% temp (val + test)\n",
        "train_files, temp_files = train_test_split(\n",
        "    all_files, test_size=0.4, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "# Second split: 50% val, 50% test from the remaining 40%\n",
        "val_files, test_files = train_test_split(\n",
        "    temp_files, test_size=0.5, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "train_files = train_files[:DATA_SUBSAMPLE]\n",
        "\n",
        "print(f\"Train: {len(train_files)}, Val: {len(val_files)}, Test: {len(test_files)}\")\n",
        "\n",
        "# Load features and labels\n",
        "X_train, Y_train = read_files(train_files, TARGET_CLASSES)\n",
        "X_val, Y_val = read_files(val_files, TARGET_CLASSES)\n",
        "X_test, Y_test = read_files(test_files, TARGET_CLASSES)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d74cc19c-179f-4f4d-9f33-1a1a1b435322",
      "metadata": {
        "id": "d74cc19c-179f-4f4d-9f33-1a1a1b435322"
      },
      "source": [
        "## Evaluation Functions (Metrics & Cost)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ef0611a-c247-414d-9144-55c6eb5a0e11",
      "metadata": {
        "id": "7ef0611a-c247-414d-9144-55c6eb5a0e11"
      },
      "outputs": [],
      "source": [
        "# Flatten: Each frame is a sample\n",
        "def flatten_for_framewise_classification(X, Y_class):\n",
        "    X_flat = np.concatenate(X)  # shape: (total_frames, num_features)\n",
        "    Y_flat = np.concatenate(Y_class)  # shape: (total_frames,)\n",
        "    return X_flat, Y_flat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2901dea3-c8bf-4d8e-8282-7f60f082bbd3",
      "metadata": {
        "id": "2901dea3-c8bf-4d8e-8282-7f60f082bbd3"
      },
      "outputs": [],
      "source": [
        "def evaluate_classifiers(\n",
        "    classes: list[str],\n",
        "    Y_val: dict[str, list[np.ndarray]],\n",
        "    X_val: list[np.ndarray] = None,\n",
        "    inference_funcs: dict[str, callable] = None,\n",
        "    Y_pred: dict[str, list[np.ndarray]] = None\n",
        ") -> tuple[dict[str, list[np.ndarray]], dict[str, dict]]:\n",
        "    \"\"\"\n",
        "    Evaluates per-frame binary classifiers and computes metrics per class.\n",
        "    Uses either computed predictions or given inference functions.\n",
        "\n",
        "    Args:\n",
        "        classes: List of class names to evaluate.\n",
        "        Y_val: Dict mapping class names to lists of ground-truth (T,) binary arrays.\n",
        "        X_val: List of input feature arrays, one per validation file. Required if Y_pred not given.\n",
        "        inference_funcs: Dict mapping class names to binary inference functions.\n",
        "        Y_pred: Dict with precomputed predictions (same format as Y_val).\n",
        "\n",
        "    Returns:\n",
        "        metrics: Dict[class → {'balanced_accuracy', 'precision', 'recall', 'f1'}].\n",
        "    \"\"\"\n",
        "\n",
        "    if Y_pred is None:\n",
        "        assert inference_funcs is not None and X_val is not None, \"If 'Y_pred' is not given, 'inference_funcs' \\\n",
        "                                                                    and 'X_val' must be given.\"\n",
        "\n",
        "    Y_val_preds = {}\n",
        "    metrics     = {}\n",
        "\n",
        "    for cls in classes:\n",
        "        # use predictions if given, else infer\n",
        "        if Y_pred and cls in Y_pred:\n",
        "            preds_per_file = Y_pred[cls]\n",
        "        else:\n",
        "            infer = inference_funcs[cls]\n",
        "            preds_per_file = [infer(x_file) for x_file in X_val]\n",
        "        Y_val_preds[cls] = preds_per_file\n",
        "\n",
        "        # flatten to compute metrics\n",
        "        y_true = np.concatenate(Y_val[cls])\n",
        "        y_pred = np.concatenate(preds_per_file)\n",
        "\n",
        "        metrics[cls] = {\n",
        "            \"balanced_accuracy\": balanced_accuracy_score(y_true, y_pred),\n",
        "            \"precision\":         precision_score(y_true, y_pred, zero_division=0),\n",
        "            \"recall\":            recall_score(y_true, y_pred, zero_division=0),\n",
        "            \"f1\":                f1_score(y_true, y_pred, zero_division=0),\n",
        "        }\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "056494f9-3ae5-46b3-ad5b-27211b076835",
      "metadata": {
        "id": "056494f9-3ae5-46b3-ad5b-27211b076835"
      },
      "outputs": [],
      "source": [
        "def evaluate_cost(\n",
        "    val_files: list[str],\n",
        "    dataset_path: str,\n",
        "    classes: list[str],\n",
        "    X_val: list[np.ndarray] = None,\n",
        "    inference_funcs: dict[str, callable] = None,\n",
        "    Y_pred: dict[str, list[np.ndarray]] = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Computes segment-level cost based on predictions and ground truth.\n",
        "    Uses either computed predictions or given inference functions.\n",
        "\n",
        "    Args:\n",
        "        val_files: List of filenames corresponding to X_val.\n",
        "        dataset_path: Path to dataset root (used for loading ground truth).\n",
        "        classes: List of class names to evaluate.\n",
        "        X_val: List of input feature arrays, one per validation file. Required if Y_pred not given.\n",
        "        inference_funcs: Dict mapping class names to binary inference functions.\n",
        "        Y_pred: Dict with precomputed predictions (class → list of (T,) arrays).\n",
        "\n",
        "    Returns:\n",
        "        total: Total cost across all validation files.\n",
        "        breakdown: Dict[class → segment-level cost].\n",
        "    \"\"\"\n",
        "\n",
        "    if Y_pred is None:\n",
        "        assert inference_funcs is not None and X_val is not None, \"If 'Y_pred' is not given, 'inference_funcs' \\\n",
        "                                                                    and 'X_val' must be given.\"\n",
        "\n",
        "    # 0) frame-wise predictions (per class)\n",
        "    if Y_pred is None:\n",
        "        Y_pred = {\n",
        "            cls: [infer(x_file) for x_file in X_val]\n",
        "            for cls, infer in inference_funcs.items()\n",
        "        }\n",
        "\n",
        "    # 1) restructure to filename -> class -> (T,) array\n",
        "    preds_by_file = {}\n",
        "    for i, fname in enumerate(val_files):\n",
        "        preds_by_file[fname] = {\n",
        "            cls: Y_pred[cls][i] for cls in classes\n",
        "        }\n",
        "\n",
        "    # 2) segment-level aggregation using compute_cost\n",
        "    pred_df = get_segment_prediction_df(\n",
        "        predictions=preds_by_file,\n",
        "        class_names=classes\n",
        "    )\n",
        "\n",
        "    # 3) load & aggregate ground truth using compute_cost\n",
        "    gt_df = get_ground_truth_df(val_files, dataset_path)\n",
        "\n",
        "    # 4) sanity checks from compute_cost\n",
        "    check_dataframe(pred_df, dataset_path)\n",
        "    check_dataframe(gt_df, dataset_path)\n",
        "\n",
        "    # 5) compute cost\n",
        "    total, breakdown = total_cost(pred_df, gt_df)\n",
        "\n",
        "    return total, breakdown"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3de36779-7ede-49c1-9c6e-ceb3b205bc23",
      "metadata": {
        "id": "3de36779-7ede-49c1-9c6e-ceb3b205bc23"
      },
      "source": [
        "## Most-Frequent Label Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56a3c1f1-eb47-4f70-b8d0-e5c2804acefb",
      "metadata": {
        "id": "56a3c1f1-eb47-4f70-b8d0-e5c2804acefb"
      },
      "outputs": [],
      "source": [
        "def baseline_most_frequent(\n",
        "    Y_train: dict[str, list[np.ndarray]],\n",
        "    classes: list[str]\n",
        ") -> dict[str, callable]:\n",
        "    \"\"\"\n",
        "    Returns inference functions that always predict each class’s majority label.\n",
        "    \"\"\"\n",
        "    inference_funcs = {}\n",
        "    for cls in classes:\n",
        "        all_frames = np.concatenate(Y_train[cls])\n",
        "        most_freq_label  = int(np.mean(all_frames) >= 0.5)\n",
        "        # inference func ignores features, just returns most frequent label per frame\n",
        "        inference_funcs[cls] = lambda x, ml=most_freq_label: np.full(x.shape[0], ml, dtype=int)\n",
        "    return inference_funcs\n",
        "\n",
        "# 1) Create baseline’s inference functions\n",
        "bl_inference_funcs = baseline_most_frequent(Y_train, TARGET_CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ae53171-36b1-4769-ad8d-f4dea911e25a",
      "metadata": {
        "id": "4ae53171-36b1-4769-ad8d-f4dea911e25a"
      },
      "outputs": [],
      "source": [
        "# metrics for most-frequent label baseline\n",
        "val_metrics = evaluate_classifiers(\n",
        "    classes=TARGET_CLASSES,\n",
        "    X_val=X_val,\n",
        "    Y_val=Y_val,\n",
        "    inference_funcs=bl_inference_funcs\n",
        ")\n",
        "\n",
        "df = pd.DataFrame(val_metrics).T.round(3)\n",
        "df.columns = [\"BAcc\", \"Precision\", \"Recall\", \"F1\"]\n",
        "print(tabulate(df, headers='keys', tablefmt='github'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0392882d-960f-433f-8211-efb923d5235a",
      "metadata": {
        "id": "0392882d-960f-433f-8211-efb923d5235a"
      },
      "outputs": [],
      "source": [
        "# cost for most-frequent label baseline\n",
        "total, breakdown = evaluate_cost(\n",
        "    val_files=val_files,\n",
        "    dataset_path=DATASET_PATH,\n",
        "    classes=TARGET_CLASSES,\n",
        "    X_val=X_val,\n",
        "    inference_funcs=bl_inference_funcs\n",
        ")\n",
        "\n",
        "df = pd.DataFrame({cls: {\"Avg. Cost per minute\": round(m[\"cost\"], 4)} for cls, m in breakdown.items()}).T\n",
        "print(f\"Total average cost per minute: {total:.4f}\\n\")\n",
        "print(tabulate(df, headers=\"keys\", tablefmt=\"github\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71147f49-db70-4a1c-8ab7-08cafb1ec4fe",
      "metadata": {
        "id": "71147f49-db70-4a1c-8ab7-08cafb1ec4fe"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7b2fec2-e8cd-4eac-bf58-3cc4ad9f9983",
      "metadata": {
        "id": "d7b2fec2-e8cd-4eac-bf58-3cc4ad9f9983"
      },
      "outputs": [],
      "source": [
        "def train_logistic_regression(\n",
        "    X_train: list[np.ndarray],\n",
        "    Y_train: dict[str, list[np.ndarray]],\n",
        "    classes: list[str]\n",
        ") -> dict[str, callable]:\n",
        "    \"\"\"\n",
        "    Trains one scaler+logistic-regression per class and returns a dict of\n",
        "    inference functions. Each function takes a (T, D) feature array and\n",
        "    returns a (T,) array of {0,1} predictions.\n",
        "    \"\"\"\n",
        "    inference_funcs = {}\n",
        "    for cls in classes:\n",
        "        # prepare frame-wise training data\n",
        "        X_tr, y_tr = flatten_for_framewise_classification(X_train, Y_train[cls])\n",
        "\n",
        "        # fit scaler and model\n",
        "        scaler = StandardScaler().fit(X_tr)\n",
        "        X_tr_scaled = scaler.transform(X_tr)\n",
        "        clf = LogisticRegression(\n",
        "            max_iter=100,\n",
        "            class_weight='balanced',\n",
        "            random_state=42\n",
        "        ).fit(X_tr_scaled, y_tr)\n",
        "\n",
        "        # define and store the joined inference function\n",
        "        def make_inference(scaler, clf):\n",
        "            return lambda x: clf.predict(scaler.transform(x))\n",
        "\n",
        "        inference_funcs[cls] = make_inference(scaler, clf)\n",
        "\n",
        "    return inference_funcs\n",
        "\n",
        "lr_inference_funcs = train_logistic_regression(\n",
        "    X_train, Y_train, TARGET_CLASSES\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc5a1d0e-24b0-40b9-b1b0-1e2ed1785a3b",
      "metadata": {
        "id": "dc5a1d0e-24b0-40b9-b1b0-1e2ed1785a3b"
      },
      "outputs": [],
      "source": [
        "val_metrics = evaluate_classifiers(\n",
        "    classes=TARGET_CLASSES,\n",
        "    X_val=X_val,\n",
        "    Y_val=Y_val,\n",
        "    inference_funcs=lr_inference_funcs\n",
        ")\n",
        "\n",
        "df = pd.DataFrame(val_metrics).T.round(3)\n",
        "df.columns = [\"BAcc\", \"Precision\", \"Recall\", \"F1\"]\n",
        "print(tabulate(df, headers='keys', tablefmt='github'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1c2c598-5a86-4c00-85fb-9ee02f4bf91d",
      "metadata": {
        "id": "b1c2c598-5a86-4c00-85fb-9ee02f4bf91d"
      },
      "outputs": [],
      "source": [
        "# inference_funcs from train_logistic_regression_inference(...)\n",
        "total, breakdown = evaluate_cost(\n",
        "    val_files=val_files,\n",
        "    dataset_path=DATASET_PATH,\n",
        "    classes=TARGET_CLASSES,\n",
        "    X_val=X_val,\n",
        "    inference_funcs=lr_inference_funcs\n",
        ")\n",
        "\n",
        "df = pd.DataFrame({cls: {\"Avg. Cost per minute\": round(m[\"cost\"], 4)} for cls, m in breakdown.items()}).T\n",
        "print(f\"Total average cost per minute: {total:.4f}\\n\")\n",
        "print(tabulate(df, headers=\"keys\", tablefmt=\"github\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb27302a-edcf-4880-bda3-3476ad025b95",
      "metadata": {
        "id": "fb27302a-edcf-4880-bda3-3476ad025b95"
      },
      "source": [
        "# Bidirectional Gated Recurrent Unit\n",
        "\n",
        "Can a recurrent neural network, which models temporal dependencies across frames, outperform logistic regression, which treats each frame independently, in sound event detection?\n",
        "\n",
        "We will implement the required ingredients in the following order:\n",
        "* Dataset\n",
        "* DataModule\n",
        "* RNN Model\n",
        "* PyTorch Lightning Module\n",
        "* Hyperparameter Configuration\n",
        "* Logging via Weights & Biases\n",
        "* Callbacks\n",
        "* PyTorch Lightning Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d8c9743-4a32-4b74-81bc-5c56a76a8784",
      "metadata": {
        "id": "6d8c9743-4a32-4b74-81bc-5c56a76a8784"
      },
      "source": [
        "## Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9618fcfc-b5eb-4b66-be1c-a04563b73b57",
      "metadata": {
        "id": "9618fcfc-b5eb-4b66-be1c-a04563b73b57"
      },
      "outputs": [],
      "source": [
        "class SequenceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for sequence modeling tasks with optional per-frame binary labels.\n",
        "\n",
        "    Args:\n",
        "        X: List of input feature arrays (T_i, D), one per file.\n",
        "        Y: Optional dict[class → list of (T_i,) label arrays], one per file and class.\n",
        "        classes: List of class names to extract from Y.\n",
        "        filenames: List of filenames corresponding to each input.\n",
        "\n",
        "    Returns:\n",
        "        Each item is a tuple:\n",
        "        - (features, labels, filename): if Y is provided\n",
        "        - (features, filename): if Y is None\n",
        "    \"\"\"\n",
        "    def __init__(self, X, Y, classes, filenames):\n",
        "      # in colab with limited RAM, we convert our files to\n",
        "      # tensors only in __getitem__\n",
        "      self.X = X  # Keep X as a list of np.ndarrays\n",
        "      self.Y = Y\n",
        "      self.classes = classes\n",
        "      self.filenames = filenames\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x_tensor = torch.tensor(self.X[idx], dtype=torch.float32)  # Convert on access\n",
        "        if self.Y is not None:\n",
        "            y_tensor = torch.stack([\n",
        "                torch.tensor(self.Y[c][idx], dtype=torch.long) for c in self.classes\n",
        "            ], dim=1)\n",
        "            return x_tensor, y_tensor, self.filenames[idx]\n",
        "        else:\n",
        "            return x_tensor, self.filenames[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2558338c-785f-43cf-86c0-bf7c5bd35ea1",
      "metadata": {
        "id": "2558338c-785f-43cf-86c0-bf7c5bd35ea1"
      },
      "outputs": [],
      "source": [
        "ds = SequenceDataset(X_train, Y_train, TARGET_CLASSES, train_files)\n",
        "feat0, label0, file0 = ds[0]\n",
        "print(\"SequenceDataset[0] -> feature shape:\", feat0.shape,\n",
        "      \"\\nlabel shape:\", label0.shape,\n",
        "      \"\\nfile[0]:\", file0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e997c003-5389-4c17-8c85-4ee4df5658f4",
      "metadata": {
        "id": "e997c003-5389-4c17-8c85-4ee4df5658f4"
      },
      "outputs": [],
      "source": [
        "# collate_fn used to create batches from the individual dataset items\n",
        "def collate_fn(batch):\n",
        "    if len(batch[0]) == 3:\n",
        "        Xs, Ys, filenames = zip(*batch)\n",
        "        lengths = torch.tensor([x.size(0) for x in Xs], dtype=torch.long)\n",
        "        X_padded = pad_sequence(Xs, batch_first=True)\n",
        "        Y_padded = pad_sequence(Ys, batch_first=True)\n",
        "        return X_padded, Y_padded, lengths, list(filenames)\n",
        "    elif len(batch[0]) == 2:\n",
        "        Xs, filenames = zip(*batch)\n",
        "        lengths = torch.tensor([x.size(0) for x in Xs], dtype=torch.long)\n",
        "        X_padded = pad_sequence(Xs, batch_first=True)\n",
        "        return X_padded, lengths, list(filenames)\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected batch format: expected 2 or 3 elements per item.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9ef9688-d431-45b0-a9ed-38396285dc23",
      "metadata": {
        "id": "c9ef9688-d431-45b0-a9ed-38396285dc23"
      },
      "outputs": [],
      "source": [
        "batch = [ds[i] for i in range(32)]\n",
        "X_pad, Y_pad, lengths, filenames = collate_fn(batch)\n",
        "\n",
        "print(\"collate_fn -> X_padded:\", X_pad.shape,\n",
        "      \"\\nY_padded:\", Y_pad.shape,\n",
        "      \"\\nlengths:\", lengths,\n",
        "      \"\\nfilenames:\", filenames[:3], \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PFRH4Po61H2Y",
      "metadata": {
        "id": "PFRH4Po61H2Y"
      },
      "source": [
        "## DataModule\n",
        "\n",
        "A `LightningDataModule` which organizes **all data loading logic** in one place.\n",
        "\n",
        "Implements the following core API.\n",
        "\n",
        "| Method                 | Purpose                                |\n",
        "|------------------------|----------------------------------------|\n",
        "| `__init__()`           | Save paths, batch size, classes, etc.  |\n",
        "| `setup(stage)`         | Prepare datasets (train/val/test)      |\n",
        "| `train_dataloader()`   | Return DataLoader for training         |\n",
        "| `val_dataloader()`     | Return DataLoader for validation       |\n",
        "| `test_dataloader()`    | Return DataLoader for testing          |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "005fd14a-6771-4b27-ba6f-11a1cc262a9a",
      "metadata": {
        "id": "005fd14a-6771-4b27-ba6f-11a1cc262a9a"
      },
      "outputs": [],
      "source": [
        "# DataModule is used by pytorch lightning\n",
        "class SEDDataModule(pl.LightningDataModule):\n",
        "    def __init__(self,\n",
        "                 X_train, Y_train, train_files,\n",
        "                 X_val,   Y_val,   val_files,\n",
        "                 X_test,  Y_test,  test_files,\n",
        "                 classes,\n",
        "                 batch_size=32,\n",
        "                 num_workers=4):\n",
        "        super().__init__()\n",
        "        self.X_train, self.Y_train, self.train_files = X_train, Y_train, train_files\n",
        "        self.X_val,   self.Y_val,   self.val_files   = X_val,   Y_val,   val_files\n",
        "        self.X_test,  self.Y_test,  self.test_files  = X_test,  Y_test,  test_files\n",
        "        self.classes     = classes\n",
        "        self.batch_size  = batch_size\n",
        "        self.num_workers = num_workers\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.train_ds = SequenceDataset(self.X_train, self.Y_train, self.classes, self.train_files)\n",
        "        self.val_ds   = SequenceDataset(self.X_val,   self.Y_val,   self.classes, self.val_files)\n",
        "        self.test_ds  = SequenceDataset(self.X_test,  self.Y_test,  self.classes, self.test_files)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_ds,\n",
        "                          batch_size=self.batch_size,\n",
        "                          shuffle=True,\n",
        "                          collate_fn=collate_fn,\n",
        "                          num_workers=self.num_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_ds,\n",
        "                          batch_size=self.batch_size,\n",
        "                          shuffle=False,\n",
        "                          collate_fn=collate_fn,\n",
        "                          num_workers=self.num_workers)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_ds,\n",
        "                          batch_size=self.batch_size,\n",
        "                          shuffle=False,\n",
        "                          collate_fn=collate_fn,\n",
        "                          num_workers=self.num_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1094b704-7922-4e35-ac8f-6613b9758322",
      "metadata": {
        "id": "1094b704-7922-4e35-ac8f-6613b9758322"
      },
      "outputs": [],
      "source": [
        "dm = SEDDataModule(\n",
        "    X_train=X_train, Y_train=Y_train, train_files=train_files,\n",
        "    X_val=X_val,     Y_val=Y_val,     val_files=val_files,\n",
        "    X_test=X_test,   Y_test=Y_test,   test_files=test_files,\n",
        "    classes=TARGET_CLASSES,\n",
        "    batch_size=32,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "dm.setup()\n",
        "loader = dm.train_dataloader()\n",
        "X_batch, Y_batch, len_batch, filenames = next(iter(loader))\n",
        "print(\"DataModule batch -> X:\", X_batch.shape,\n",
        "      \"\\nY:\", Y_batch.shape,\n",
        "      \"\\nlengths:\", len_batch,\n",
        "      \"\\nfilenames:\", filenames[:3], \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92ef17e1-234f-4c11-a278-129a6cbe5188",
      "metadata": {
        "id": "92ef17e1-234f-4c11-a278-129a6cbe5188"
      },
      "source": [
        "### Bidirectional RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea557706-85dd-435c-87cc-4198ce27523d",
      "metadata": {
        "id": "ea557706-85dd-435c-87cc-4198ce27523d"
      },
      "outputs": [],
      "source": [
        "class BiGRUClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Bidirectional GRU classifier with a linear output layer.\n",
        "\n",
        "    Args:\n",
        "        input_dim: Input feature dimension (D).\n",
        "        hidden_dim: Hidden size per GRU direction.\n",
        "        num_layers: Number of stacked GRU layers.\n",
        "        num_classes: Number of output classes (C).\n",
        "\n",
        "    Input:\n",
        "        x: Tensor of shape (B, T, D) — batch of padded sequences.\n",
        "        lengths: Tensor of shape (B,) — actual lengths before padding.\n",
        "\n",
        "    Returns:\n",
        "        logits: Tensor of shape (B, T, C) — class scores for each time step.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.classifier = nn.Linear(hidden_dim * 2, num_classes)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        # x: (B, T, D), lengths: (B,)\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(\n",
        "            x, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "        packed_out, _ = self.gru(packed)\n",
        "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
        "        # out: (B, T, 2*hidden_dim)\n",
        "        logits = self.classifier(out)  # (B, T, num_classes)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7fc891a-7ed3-41be-a26f-d2f77b360bfe",
      "metadata": {
        "id": "e7fc891a-7ed3-41be-a26f-d2f77b360bfe"
      },
      "outputs": [],
      "source": [
        "# Instantiate model\n",
        "model = BiGRUClassifier(\n",
        "    input_dim=X_batch.shape[-1],\n",
        "    hidden_dim=1024,\n",
        "    num_layers=2,\n",
        "    num_classes=Y_batch.shape[-1]\n",
        ")\n",
        "\n",
        "# Forward pass\n",
        "logits = model(X_batch, len_batch)\n",
        "\n",
        "# Print shapes\n",
        "print(\"Input X_batch shape:\", X_batch.shape)       # (B, T_max, F)\n",
        "print(\"Output logits shape:\", logits.shape)         # (B, T_max, C)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d89c476-715a-4118-8cf2-2f7c126e69d7",
      "metadata": {
        "id": "0d89c476-715a-4118-8cf2-2f7c126e69d7"
      },
      "source": [
        "### PyTorch Lightning Module: `SEDLightningModule`\n",
        "\n",
        "The `LightningModule` wraps your model and training logic, abstracting away boilerplate code and handling key training steps automatically.  \n",
        "It implements a **standardized API** to define how your model should behave during training, validation, testing, and prediction.\n",
        "\n",
        "- **`__init__`**: initializes the model (`BiGRUClassifier`), loss function, and validation and test buffer storage, and sets important attributes (e.g., lr, threshold).\n",
        "- **`forward(x, lengths)`**: forward pass through the GRU model.\n",
        "- **`predict_step(batch, batch_idx)`**: applies sigmoid + thresholding, slices off padding → returns predictions.\n",
        "- **`training_step(batch, batch_idx)`**: handles training logic.\n",
        "- **`validation_step(batch, batch_idx)`**: handles validation logic.\n",
        "- **`on_validation_epoch_end()`**: aggregates validation results after each epoch.\n",
        "- **`configure_optimizers()`**: defines the optimizer (Adam)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e2ff937-88d9-411b-b820-94c3f8106fc1",
      "metadata": {
        "id": "3e2ff937-88d9-411b-b820-94c3f8106fc1"
      },
      "outputs": [],
      "source": [
        "class SEDLightningModule(pl.LightningModule):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, classes, lr=1e-4, threshold=0.5):\n",
        "        super().__init__()\n",
        "        # Core model\n",
        "        self.model = BiGRUClassifier(\n",
        "            input_dim=input_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            num_classes=len(classes)\n",
        "        )\n",
        "\n",
        "        self.classes = classes\n",
        "\n",
        "        # Loss (we'll apply masking later, thus reduction='none')\n",
        "        self.criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
        "        self.lr = lr\n",
        "        self.threshold = threshold\n",
        "\n",
        "        self._val_preds   = {c: [] for c in self.classes}\n",
        "        self._val_targets = {c: [] for c in self.classes}\n",
        "        self._val_filenames = []\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        return self.model(x, lengths)\n",
        "\n",
        "    def predict_step(self, batch, batch_idx):\n",
        "        # unpack batch (with or without labels)\n",
        "        if len(batch) == 4:\n",
        "            X, _, lengths, filenames = batch\n",
        "        else:\n",
        "            X, lengths, filenames = batch\n",
        "\n",
        "        # 1) raw logits → probs → binary preds\n",
        "        logits = self.model(X, lengths)\n",
        "        probs  = torch.sigmoid(logits)\n",
        "        preds  = (probs > self.threshold).int() # (B, T_max, C)\n",
        "\n",
        "        # 2) remove padding\n",
        "        batch_preds = [preds[b, :lengths[b]].cpu()\n",
        "                      for b in range(X.size(0))]\n",
        "\n",
        "        return {\"filenames\": filenames, \"preds\": batch_preds}\n",
        "\n",
        "    # we will implement the processing steps one after the other in the following\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self.process_training_step(batch, batch_idx)\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        return self.process_validation_step(batch, batch_idx)\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        return self.process_validation_epoch_end()\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        return self.process_test_step(batch, batch_idx)\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        return self.process_test_epoch_end()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LaIEwXuR1doq",
      "metadata": {
        "id": "LaIEwXuR1doq"
      },
      "source": [
        "### PyTorch Lightning Module: `SEDLightningModule`\n",
        "\n",
        "In the following, we will implement the missing functions:\n",
        "\n",
        "- `process_training_step`:  \n",
        "  computes masked BCE loss for each frame and logs the training loss.\n",
        "\n",
        "- `process_validation_step`:  \n",
        "  collects per-frame predictions and targets for later metric computation.\n",
        "\n",
        "- `process_validation_epoch_end`:  \n",
        "  aggregates predictions and targets, computes metrics, and logs results.\n",
        "\n",
        "- `process_test_step`:  \n",
        "  same as validation but used for test-time evaluation.\n",
        "\n",
        "- `process_test_epoch_end`:  \n",
        "  evaluates and logs performance after test epoch.\n",
        "\n",
        "We will bind this functions to our `SEDLightningModule`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V1fIujPB1fuu",
      "metadata": {
        "id": "V1fIujPB1fuu"
      },
      "source": [
        "### `process_training_step`\n",
        "\n",
        "computes masked BCE loss for each frame and logs the training loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50e53310-367c-4d4c-b905-d4aa74ccee95",
      "metadata": {
        "id": "50e53310-367c-4d4c-b905-d4aa74ccee95"
      },
      "outputs": [],
      "source": [
        "def process_training_step(self, batch, batch_idx):\n",
        "    X, Y, lengths, _ = batch      # X: (B, T, D), Y: (B, T, C), lengths: (B,)\n",
        "    logits = self(X, lengths)     # calls self.forward, results in logits of shape (B, T, C)\n",
        "\n",
        "    # raw per-element loss\n",
        "    loss_raw = self.criterion(logits, Y.float())  # (B, T, C)\n",
        "\n",
        "    # build mask to zero out padded frames\n",
        "    mask = torch.arange(logits.size(1), device=logits.device)[None, :] < lengths[:, None]\n",
        "    mask = mask.unsqueeze(-1).float()     # (B, T, 1)\n",
        "\n",
        "    # apply mask and average\n",
        "    loss = (loss_raw * mask).sum() / mask.sum()\n",
        "\n",
        "    self.log('train/loss', loss, prog_bar=True, on_step=True, on_epoch=True, batch_size=X.size(0))\n",
        "    return loss\n",
        "\n",
        "# Bind it to the LightningModule\n",
        "SEDLightningModule.process_training_step = process_training_step"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TDnNLDPC1iBm",
      "metadata": {
        "id": "TDnNLDPC1iBm"
      },
      "source": [
        "### `process_validation_step`\n",
        "\n",
        "computes masked BCE loss, logs it, and stores frame-level predictions and targets for aggreation in `process_validation_epoch_end`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a50181aa-442c-4a1a-84ee-b50135a78d7b",
      "metadata": {
        "id": "a50181aa-442c-4a1a-84ee-b50135a78d7b"
      },
      "outputs": [],
      "source": [
        "def process_validation_step(self, batch, batch_idx):\n",
        "    X, Y, lengths, filenames = batch      # X: (B, T, D), Y: (B, T, C), lengths: (B,)\n",
        "    logits = self(X, lengths)             # calls self.forward, results in logits of shape (B, T, C)\n",
        "\n",
        "    # Determine logging prefix\n",
        "    prefix = \"test\" if self.trainer.testing else \"val\"\n",
        "\n",
        "    # compute masked BCE loss\n",
        "    loss_raw = self.criterion(logits, Y.float())     # (B, T, C)\n",
        "    mask = torch.arange(logits.size(1), device=logits.device)[None, :] < lengths[:, None]\n",
        "    mask = mask.unsqueeze(-1).float()                # (B, T, 1)\n",
        "    loss = (loss_raw * mask).sum() / mask.sum()\n",
        "\n",
        "    self.log(f'{prefix}/loss', loss, prog_bar=True, on_step=False, on_epoch=True, batch_size=X.size(0))\n",
        "\n",
        "    # store frame-wise preds & targets for epoch_end\n",
        "    # frame-wise logits are thresholded here\n",
        "    preds = (torch.sigmoid(logits) > self.threshold).long()     # (B, T, C)\n",
        "    self._val_filenames.extend(filenames)\n",
        "\n",
        "    for i, c in enumerate(self.classes):\n",
        "        for b in range(X.size(0)):\n",
        "            T = lengths[b]\n",
        "            self._val_preds[c].append(preds[b, :T, i])\n",
        "            self._val_targets[c].append(Y[b, :T, i])\n",
        "\n",
        "    return loss\n",
        "\n",
        "# Bind it to the LightningModule\n",
        "SEDLightningModule.process_validation_step = process_validation_step"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Cp5PXzjG1oNB",
      "metadata": {
        "id": "Cp5PXzjG1oNB"
      },
      "source": [
        "### `process_validation_epoch_end`\n",
        "\n",
        "computes dataset metrics and cost and logs them at the end of a validation epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4760117-0ed4-44da-9d99-39165f325ef9",
      "metadata": {
        "id": "f4760117-0ed4-44da-9d99-39165f325ef9"
      },
      "outputs": [],
      "source": [
        "def process_validation_epoch_end(self):\n",
        "    # Determine current mode\n",
        "    prefix = \"test\" if self.trainer.testing else \"val\"\n",
        "\n",
        "    # --- 1) Convert buffered tensors to NumPy arrays ---\n",
        "    preds_numpy = {\n",
        "        cls: [p.cpu().numpy() for p in self._val_preds[cls]]\n",
        "        for cls in self.classes\n",
        "    }\n",
        "    targets_numpy = {\n",
        "        cls: [t.cpu().numpy() for t in self._val_targets[cls]]\n",
        "        for cls in self.classes\n",
        "    }\n",
        "\n",
        "    # --- 2) Frame‐level metrics ---\n",
        "    frame_metrics = evaluate_classifiers(\n",
        "        classes=self.classes,\n",
        "        Y_val=targets_numpy,\n",
        "        Y_pred=preds_numpy\n",
        "    )\n",
        "\n",
        "    for cls, m in frame_metrics.items():\n",
        "        self.log(f'{prefix}/{cls}_bacc',     m['balanced_accuracy'])\n",
        "        self.log(f'{prefix}/{cls}_precision',m['precision'])\n",
        "        self.log(f'{prefix}/{cls}_recall',   m['recall'])\n",
        "        self.log(f'{prefix}/{cls}_f1',       m['f1'])\n",
        "\n",
        "    # --- 3) Segment‐level cost ---\n",
        "    total_cost, cost_breakdown = evaluate_cost(\n",
        "        val_files=self._val_filenames,\n",
        "        dataset_path=DATASET_PATH,\n",
        "        classes=self.classes,\n",
        "        Y_pred=preds_numpy\n",
        "    )\n",
        "    self.log(f'{prefix}/total_cost', total_cost, prog_bar=True)\n",
        "    for cls, cls_cost in cost_breakdown.items():\n",
        "        self.log(f\"{prefix}/cost/{cls}\", cls_cost[\"cost\"], prog_bar=False)\n",
        "\n",
        "    # --- 4) Clear buffers ---\n",
        "    self._val_preds     = {c: [] for c in self.classes}\n",
        "    self._val_targets   = {c: [] for c in self.classes}\n",
        "    self._val_filenames = []\n",
        "\n",
        "\n",
        "SEDLightningModule.process_validation_epoch_end = process_validation_epoch_end"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fjcoVY4J1vaE",
      "metadata": {
        "id": "fjcoVY4J1vaE"
      },
      "source": [
        "### `process_test_step` and `process_test_epoch_end` ...\n",
        "\n",
        "fortunately require the same logic as validation, so we can reuse `process_validation_step` and `process_validation_epoch_end`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80675367-4740-4ff2-b9cb-73bcf92bb40e",
      "metadata": {
        "id": "80675367-4740-4ff2-b9cb-73bcf92bb40e"
      },
      "outputs": [],
      "source": [
        "# After you’ve attached the validation logic, simply reuse it for testing:\n",
        "\n",
        "# Reuse the same step‐logic\n",
        "SEDLightningModule.process_test_step = SEDLightningModule.process_validation_step\n",
        "\n",
        "# Reuse the same epoch‐end logic\n",
        "SEDLightningModule.process_test_epoch_end = SEDLightningModule.process_validation_epoch_end"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "881bf9dc-fb5a-47c9-84c1-abcd9bb79053",
      "metadata": {
        "id": "881bf9dc-fb5a-47c9-84c1-abcd9bb79053"
      },
      "source": [
        "### Hyperparameters\n",
        "\n",
        "Key hyperparameters with reasonable initial values — most likely not guaranteed optimal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b164a252-b4eb-4279-8d98-b06cd85a05f8",
      "metadata": {
        "id": "b164a252-b4eb-4279-8d98-b06cd85a05f8"
      },
      "outputs": [],
      "source": [
        "hparams = dict(\n",
        "    # not tuned by us - used out of the box\n",
        "    input_dim      = X_batch.shape[-1],\n",
        "    hidden_dim     = 1024,\n",
        "    num_layers     = 2,\n",
        "    lr             = 1e-4,\n",
        "    batch_size     = 64,\n",
        "    max_epochs     = 50,\n",
        "    threshold      = 0.5,\n",
        "    patience       = 5,         # Early-stopping patience\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c9882b9-ffdb-45d7-a979-a9675877455c",
      "metadata": {
        "id": "7c9882b9-ffdb-45d7-a979-a9675877455c"
      },
      "source": [
        "### Callbacks\n",
        "\n",
        "Callbacks are modular hooks that enable custom actions during training (e.g., saving checkpoints, early stopping, or logging), triggered at specific stages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37a84403-bd34-4c72-b36a-f0453b72505b",
      "metadata": {
        "id": "37a84403-bd34-4c72-b36a-f0453b72505b"
      },
      "outputs": [],
      "source": [
        "checkpoint_cb = ModelCheckpoint(\n",
        "    monitor    = \"val/total_cost\",   # minimize cost\n",
        "    mode       = \"min\",\n",
        "    save_top_k = 1,                  # save top model on validation data\n",
        "    filename   = \"best-{epoch:02d}\"\n",
        ")\n",
        "\n",
        "early_stop_cb = EarlyStopping(\n",
        "    monitor  = \"val/total_cost\",\n",
        "    mode     = \"min\",\n",
        "    patience = hparams[\"patience\"],\n",
        "    verbose  = True\n",
        ")\n",
        "\n",
        "lr_monitor_cb = LearningRateMonitor(logging_interval=\"epoch\")\n",
        "\n",
        "# RichProgressBar generates minimal output compared to 'tqdm'\n",
        "progress_bar_cb = RichProgressBar()\n",
        "\n",
        "callbacks = [checkpoint_cb, early_stop_cb, lr_monitor_cb, progress_bar_cb]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d71798b0-10fa-4bc5-9ce1-e4c72d6e0b4a",
      "metadata": {
        "id": "d71798b0-10fa-4bc5-9ce1-e4c72d6e0b4a"
      },
      "source": [
        "### Logger\n",
        "\n",
        "- [Weights & Biases (wandb)](https://wandb.ai/site/) is a powerful and free experiment tracking tool  \n",
        "- lets you log metrics, visualize training runs, compare models  \n",
        "- share results via an interactive online dashboard  \n",
        "- integrates seamlessly with PyTorch Lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42a2ebc8-a1aa-4f1a-b4f8-92559b8ced8f",
      "metadata": {
        "id": "42a2ebc8-a1aa-4f1a-b4f8-92559b8ced8f"
      },
      "outputs": [],
      "source": [
        "wandb_logger = WandbLogger(\n",
        "    project     = \"mlpc2025-sed\",\n",
        "    name        = f\"BiGRU-{hparams['hidden_dim']}x{hparams['num_layers']}\",\n",
        "    config      = hparams\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c23eb9c-f64f-4140-8dc2-2ae8a138d749",
      "metadata": {
        "id": "3c23eb9c-f64f-4140-8dc2-2ae8a138d749"
      },
      "source": [
        "### Trainer\n",
        "\n",
        "The `Trainer` is the central PyTorch Lightning component that orchestrates training, validation, and testing.\n",
        "\n",
        "It brings everything together:\n",
        "- The `SEDDataModule` provides the data.\n",
        "- The `SEDLightningModule` defines the model and training logic.\n",
        "- The `Trainer` handles the training loop, evaluation, logging, and callbacks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1258b445-f5b3-4280-a083-63881d2ba624",
      "metadata": {
        "id": "1258b445-f5b3-4280-a083-63881d2ba624"
      },
      "outputs": [],
      "source": [
        "dm = SEDDataModule(\n",
        "    X_train=X_train, Y_train=Y_train, train_files=train_files,\n",
        "    X_val=X_val,     Y_val=Y_val,     val_files=val_files,\n",
        "    X_test=X_test,   Y_test=Y_test,   test_files=test_files,\n",
        "    classes=TARGET_CLASSES,\n",
        "    batch_size=hparams[\"batch_size\"],\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "model = SEDLightningModule(\n",
        "    input_dim  = hparams[\"input_dim\"],\n",
        "    hidden_dim = hparams[\"hidden_dim\"],\n",
        "    num_layers = hparams[\"num_layers\"],\n",
        "    classes    = TARGET_CLASSES,\n",
        "    lr         = hparams[\"lr\"]\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    accelerator             = \"gpu\",\n",
        "    devices                 = 1,\n",
        "    max_epochs              = hparams[\"max_epochs\"],\n",
        "    callbacks               = callbacks,\n",
        "    logger                  = wandb_logger,\n",
        "    log_every_n_steps       = 10,\n",
        "    deterministic           = True,\n",
        "    check_val_every_n_epoch = 1,\n",
        "    num_sanity_val_steps    = 0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1aCQyLUW2GTa",
      "metadata": {
        "id": "1aCQyLUW2GTa"
      },
      "source": [
        "### Let's train!\n",
        "\n",
        "The following command launches training and validation, alternating across epochs. All results will be logged to Weights & Biases.\n",
        "You can explore a completed training run here: https://api.wandb.ai/links/cp_tobi/plk26iu9\n",
        "\n",
        "Checkpoints will stored in `mlpc2025-sed/<wandb_id>/checkpoints`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mpHZ6Ti02DHn",
      "metadata": {
        "id": "mpHZ6Ti02DHn"
      },
      "outputs": [],
      "source": [
        "trainer.fit(model, datamodule=dm)   # train and validate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5XhrlCmD2RTS",
      "metadata": {
        "id": "5XhrlCmD2RTS"
      },
      "source": [
        "### Let's test!\n",
        "\n",
        "This loads the checkpoint with the lowest validation cost and runs evaluation on the test set.\n",
        "Check example test results logged to Weights & Biases here: https://api.wandb.ai/links/cp_tobi/plk26iu9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1cm8SH92VOD",
      "metadata": {
        "id": "b1cm8SH92VOD"
      },
      "outputs": [],
      "source": [
        "test_results = trainer.test(model, datamodule=dm, ckpt_path=\"best\")   # test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a0d916d-991d-41d4-9058-12552bc33af5",
      "metadata": {
        "id": "9a0d916d-991d-41d4-9058-12552bc33af5"
      },
      "source": [
        "## Compare Baseline, Logistic Regression and BiGRU Costs on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9da56cf-9143-47d2-a927-ea32ba8a5e64",
      "metadata": {
        "id": "f9da56cf-9143-47d2-a927-ea32ba8a5e64"
      },
      "outputs": [],
      "source": [
        "# baseline inference on test set\n",
        "bl_total, bl_breakdown = evaluate_cost(\n",
        "    test_files,\n",
        "    DATASET_PATH,\n",
        "    TARGET_CLASSES,\n",
        "    X_test,\n",
        "    bl_inference_funcs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9894aefc-62fa-4b6e-9ac4-7dff12068bfe",
      "metadata": {
        "id": "9894aefc-62fa-4b6e-9ac4-7dff12068bfe"
      },
      "outputs": [],
      "source": [
        "# logistic regression inference on test set\n",
        "lr_total, lr_breakdown = evaluate_cost(\n",
        "    test_files,\n",
        "    DATASET_PATH,\n",
        "    TARGET_CLASSES,\n",
        "    X_test,\n",
        "    lr_inference_funcs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TvLufEAc2iFx",
      "metadata": {
        "id": "TvLufEAc2iFx"
      },
      "source": [
        "### Collect all costs in a pandas dataframe for pretty print"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27e9c26a-dfeb-485e-be47-84257c797a1f",
      "metadata": {
        "id": "27e9c26a-dfeb-485e-be47-84257c797a1f"
      },
      "outputs": [],
      "source": [
        "# shuffle around format for pretty print\n",
        "\n",
        "# Convert breakdowns into dict[class → cost]\n",
        "bl_costs = {cls: d[\"cost\"] for cls, d in bl_breakdown.items()}\n",
        "lr_costs = {cls: d[\"cost\"] for cls, d in lr_breakdown.items()}\n",
        "\n",
        "# Add total cost\n",
        "bl_costs[\"TOTAL\"] = bl_total\n",
        "lr_costs[\"TOTAL\"] = lr_total\n",
        "\n",
        "# Extract relevant costs from pytorch lightning test results\n",
        "rnn_result = test_results[0]\n",
        "rnn_costs = {\n",
        "    cls: rnn_result[f\"test/cost/{cls}\"]\n",
        "    for cls in TARGET_CLASSES\n",
        "    if f\"test/cost/{cls}\" in rnn_result\n",
        "}\n",
        "\n",
        "# Add total cost\n",
        "rnn_costs[\"TOTAL\"] = rnn_result[\"test/total_cost\"]\n",
        "\n",
        "# Create a DataFrame for comparison\n",
        "cost_df = pd.DataFrame({\n",
        "    \"Baseline\": bl_costs,\n",
        "    \"Logistic Regression\": lr_costs,\n",
        "    \"RNN\": rnn_costs\n",
        "}).round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c595c314-43f1-444d-9b18-83d16e843d1b",
      "metadata": {
        "id": "c595c314-43f1-444d-9b18-83d16e843d1b"
      },
      "outputs": [],
      "source": [
        "print(tabulate(cost_df.reset_index().values,\n",
        "               headers=[\"Class\", \"Baseline\", \"Logistic Regression\", \"RNN\"],\n",
        "               tablefmt=\"github\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1401d4ca-7575-4104-a344-d689eb2c1346",
      "metadata": {
        "id": "1401d4ca-7575-4104-a344-d689eb2c1346"
      },
      "source": [
        "## Compute predictions on customer's secret test set\n",
        "\n",
        "Requires three functions:\n",
        "- `load_model_from_checkpoint`: loading the desired model checkpoint, checkpoints are in folder `mlpc2025-sed/<wandb_id>/checkpoints`; an example checkpoint will be downloaded below\n",
        "- `predict_dataset`: generate predictions for customer's dataset\n",
        "- `segment_and_save`: bring predictions into the required 1.2 second segement format and save as csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95f29e59-12d3-4004-bb46-93996dd2c77d",
      "metadata": {
        "id": "95f29e59-12d3-4004-bb46-93996dd2c77d"
      },
      "outputs": [],
      "source": [
        "def load_model_from_checkpoint(\n",
        "    ckpt_path: str,\n",
        "    hparams: dict,\n",
        "    classes: list[str]\n",
        ") -> pl.LightningModule:\n",
        "    return SEDLightningModule.load_from_checkpoint(\n",
        "        checkpoint_path=ckpt_path,\n",
        "        input_dim  = hparams[\"input_dim\"],\n",
        "        hidden_dim = hparams[\"hidden_dim\"],\n",
        "        num_layers = hparams[\"num_layers\"],\n",
        "        lr         = hparams[\"lr\"],\n",
        "        threshold  = hparams[\"threshold\"],\n",
        "        classes    = classes,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5024765b-dbee-4b70-a73c-9604f754db90",
      "metadata": {
        "id": "5024765b-dbee-4b70-a73c-9604f754db90"
      },
      "outputs": [],
      "source": [
        "def predict_dataset(\n",
        "    model: pl.LightningModule,\n",
        "    loader: DataLoader\n",
        ") -> dict[str, dict[str, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Runs trainer.predict() on `loader` and returns:\n",
        "      preds_by_file[filename][class] = 1D NumPy array of frame‐wise {0,1}.\n",
        "    \"\"\"\n",
        "    trainer = pl.Trainer(accelerator=\"auto\", devices=1)\n",
        "    outputs = trainer.predict(model, dataloaders=loader)\n",
        "\n",
        "    # flatten into lists\n",
        "    all_preds = {c: [] for c in model.classes}\n",
        "    all_files = []\n",
        "    for batch_out in outputs:\n",
        "        for fname, pred in zip(batch_out[\"filenames\"], batch_out[\"preds\"]):\n",
        "            all_files.append(fname)\n",
        "            arr = pred.numpy()  # shape (T_i, C)\n",
        "            for i, cls in enumerate(model.classes):\n",
        "                all_preds[cls].append(arr[:, i])\n",
        "\n",
        "    # repackage into preds_by_file\n",
        "    preds_by_file: dict[str, dict[str, np.ndarray]] = {}\n",
        "    for idx, fname in enumerate(all_files):\n",
        "        preds_by_file.setdefault(fname, {})\n",
        "        for cls in model.classes:\n",
        "            preds_by_file[fname][cls] = all_preds[cls][idx]\n",
        "\n",
        "    return preds_by_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92be41a8-0c83-427d-b8ba-bb40faac07b1",
      "metadata": {
        "id": "92be41a8-0c83-427d-b8ba-bb40faac07b1"
      },
      "outputs": [],
      "source": [
        "def segment_and_save(\n",
        "    preds_by_file: dict[str, dict[str, np.ndarray]],\n",
        "    class_names: list[str],\n",
        "    dataset_path: str,\n",
        "    out_csv: str,\n",
        "    compute_cost: bool = False,\n",
        "    test_files: list[str] = None,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    1) Build segment‐level DataFrame\n",
        "    2) Sanity‐check with check_dataframe()\n",
        "    3) (optional) compute & print cost if val_files is provided\n",
        "    4) save CSV to out_csv\n",
        "    \"\"\"\n",
        "    # 1) aggregate predictions using the function provided in compute_cost.py\n",
        "    pred_df = get_segment_prediction_df(\n",
        "        predictions = preds_by_file,\n",
        "        class_names = class_names\n",
        "    )\n",
        "\n",
        "    # 2) sanity‐check (from compute_cost.py)\n",
        "    check_dataframe(pred_df, dataset_path)\n",
        "\n",
        "    # 3) cost (optional), for sanity check on our custom test split\n",
        "    if compute_cost and test_files is not None:\n",
        "        gt_df = get_ground_truth_df(test_files, dataset_path) # from compute_cost.py\n",
        "        total, breakdown = total_cost(pred_df, gt_df) # from compute_cost.py\n",
        "        print(f\"\\nTotal cost: {total:.4f}\")\n",
        "\n",
        "        gt_csv = os.path.splitext(out_csv)[0] + \"_ground_truth.csv\"\n",
        "        gt_df.to_csv(gt_csv, index=False)\n",
        "        print(f\"Saved ground truth segments to {gt_csv}\")\n",
        "\n",
        "    # 4) save\n",
        "    pred_df.to_csv(out_csv, index=False)\n",
        "    print(f\"Saved segment predictions to {out_csv}\")\n",
        "\n",
        "    return pred_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FmX7eosE2479",
      "metadata": {
        "id": "FmX7eosE2479"
      },
      "source": [
        "### Load checkpoint\n",
        "\n",
        "Download an example checkpoint from huggingface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Lw8ZV-lF27Jb",
      "metadata": {
        "id": "Lw8ZV-lF27Jb"
      },
      "outputs": [],
      "source": [
        "# download example checkpoint from huggingface\n",
        "ckpt_path = hf_hub_download(\n",
        "    repo_id=\"fschmid56/mlpc2025_dataset\",\n",
        "    filename=\"colab_tutorial.ckpt\",\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "\n",
        "# alternatively, use your own local checkpoint\n",
        "# replace wandb id 'lo9ygyg4' with your desired wandb id\n",
        "# replace 'best-epoch=11.ckpt' with the name of your checkpoint\n",
        "# ckpt_path = \"/content/mlpc2025-sed/lo9ygyg4/checkpoints/best-epoch=11.ckpt\"\n",
        "\n",
        "model = load_model_from_checkpoint(ckpt_path, hparams, TARGET_CLASSES)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60ujF_fv5qaS",
      "metadata": {
        "id": "60ujF_fv5qaS"
      },
      "source": [
        "We sanity check model loading, the prediction routine and segmenting predictions by applying it our custom test split and calculating costs (as we have access to the labels)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3237762a-1899-4a5c-aa11-34fdea60e39e",
      "metadata": {
        "id": "3237762a-1899-4a5c-aa11-34fdea60e39e"
      },
      "outputs": [],
      "source": [
        "# 1) TEST SPLIT\n",
        "test_dataset = SequenceDataset(X_test, Y_test, TARGET_CLASSES, test_files)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=8, collate_fn=collate_fn)\n",
        "test_preds   = predict_dataset(model, test_loader)\n",
        "segment_and_save(\n",
        "    preds_by_file = test_preds,\n",
        "    class_names   = TARGET_CLASSES,\n",
        "    dataset_path  = DATASET_PATH,\n",
        "    out_csv       = \"test_split_predictions.csv\",\n",
        "    compute_cost  = True,\n",
        "    test_files     = test_files,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_-zZiOnZ52XU",
      "metadata": {
        "id": "_-zZiOnZ52XU"
      },
      "source": [
        "Finally, compute predictions on the customer's secret test set and store as `/content/customer_predictions.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mFfP1VSk5wFq",
      "metadata": {
        "id": "mFfP1VSk5wFq"
      },
      "outputs": [],
      "source": [
        "# 2) CUSTOMER SET (no labels → compute_cost=False)\n",
        "customer_files = CUSTOMER_METADATA[\"filename\"].unique()\n",
        "X_cust, _ = read_files(customer_files, TARGET_CLASSES,\n",
        "                       features_dir=CUSTOMER_AUDIO_FEATURES_DIR,\n",
        "                       labels_dir=None)\n",
        "cust_dataset = SequenceDataset(X_cust, None, TARGET_CLASSES, customer_files)\n",
        "cust_loader  = DataLoader(cust_dataset, batch_size=8, collate_fn=collate_fn)\n",
        "\n",
        "cust_preds = predict_dataset(model, cust_loader)\n",
        "segment_and_save(\n",
        "    preds_by_file = cust_preds,\n",
        "    class_names   = TARGET_CLASSES,\n",
        "    dataset_path  = CUSTOMER_DATASET_PATH,\n",
        "    out_csv       = \"customer_predictions.csv\",\n",
        "    compute_cost  = False,  # can't compute on customer's secret test set\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14655463-963a-4b75-813f-95988535a04e",
      "metadata": {
        "id": "14655463-963a-4b75-813f-95988535a04e"
      },
      "source": [
        "### Final checks as in Task Description\n",
        "\n",
        "Instead of importing all the functions from `compute_cost.py` and working with DataFrames directly, you can also run the provided script as recommended in the Task Description. Just pass your generated .csv file(s) to verify correctness and compute cost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd94e065-d814-4edc-9b41-a1f7ff82ed95",
      "metadata": {
        "id": "cd94e065-d814-4edc-9b41-a1f7ff82ed95"
      },
      "outputs": [],
      "source": [
        "!python compute_cost.py \\\n",
        "  --dataset_path=\"{DATASET_PATH}\" \\\n",
        "  --ground_truth_csv=\"test_split_predictions_ground_truth.csv\" \\\n",
        "  --predictions_csv=\"test_split_predictions.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b234974d-c232-4e4b-8f63-3d357c69123d",
      "metadata": {
        "id": "b234974d-c232-4e4b-8f63-3d357c69123d"
      },
      "outputs": [],
      "source": [
        "!python compute_cost.py \\\n",
        "  --dataset_path=\"{CUSTOMER_DATASET_PATH}\" \\\n",
        "  --predictions_csv=\"customer_predictions.csv\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}